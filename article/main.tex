\pdfoutput=1

\documentclass[11pt]{article}
\usepackage[review]{acl}

\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{balance}
\usepackage{booktabs}
\usepackage[justification=centering]{caption}
\usepackage{cleveref}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{inconsolata}
\usepackage[utf8]{inputenc}
\usepackage{interval}
\usepackage{microtype}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{xstring}

\usepackage[true]{anonymous-acm}

\title{LLM Output Compliance with Handcrafted Linguistic Features --- An Experiment}
\authoranon{
\author{Andrei Olar\\
Universitatea Babeș-Bolyai, Cluj-Napoca, România\\
\texttt{andrei.olar@ubbcluj.ro}\\}
}

\begin{document}
\maketitle
\begin{abstract}
    Large language models are sought for solving an increasing number of
    increasingly diverse tasks. Perhaps naturally one such task is text style
    transfer: the task of changing the style of a text while preserving its
    meaning. Besides the natural approach of fine-tuning a large language model,
    a simpler and cheaper alternative should also be investigated: using
    handcrafted language features to direct the large language model.
\end{abstract}


\section{Introduction}\label{introduction}

\textbf{Large language models (LLMs)} have become a commonplace research topic
today.
Because language models such as BERT~\cite{devlin2018bert} or
GPT~\cite{gpt-2018,gpt2-2019,gpt3-2020} constantly advance the state of the art
on many natural language processing tasks, it is interesting to evaluate them on
more specialized tasks.
Multiple surveys~\cite{minaee2024llmsurvey,zhao2023survey} and
benchmarks~\cite{papcode2024hellaswag,chiang2024chatbot} show that large
language models are good at following instructions.
We can also intuit that training LLMs on diverse data (for instance the
Pile~\cite{gao2020pile}) uniquely qualifies them to produce text in a wide
variety of styles.

\textbf{Text style transfer (TST)} is defined as the ``task of transforming the
stylistic manner in which a sentence is written, while preserving the meaning of
the original sentence''~\cite{tst-review-2021}.
This definition can be extended to entire articles or corpuses of text because
these are also the object of linguistic style and
stylistics~\cite{lugea2023stylistics}.
The task of transferring text style has a certain maturity.
The interest in this task is renewed by the advancements made with LLMs.

\textbf{Handcrafted linguistic features (HLFs)} are single numerical values
produced by a uniquely identifiable method on any natural
language~\cite{lftk-2023}.
More often than not, these values are easy to compute while the methods used for
obtaining them are usually idempotent.
This makes them very attractive because of their dual potential purpose.
On one hand, they can be used to instruct the generation of text (``write no
more than 10 sentences'').
On the other hand they can be used to analyse text.
This ability allows verifying whether a text has the expected value for an HLF.

Taking Noam Chomsky's ideas~\cite{chomsky2002syntactic} that syntax and meaning
are different things, we intuit that we can define writing style only in terms
of both syntax and meaning.
We shy away from discussing meaning, but syntax and orderly constructs like it
seem to be within our grasp.

Traditionally, linguistic style was thought to be the result of morphological
and syntactic arrangements~\cite{lugea2023stylistics}.
More recent schools of thought on linguistic style find semantics and pragmatics
to be inextricably linked to author style~\cite{verma2019lexical}, but argue
that a great deal can still be inferred from lexicon, syntax and a `surface'
level which contains basic HLFs such as the number of words in a sentence.

Some have observed that many characteristics of written text can be computed as
HLFs~\cite{hovy1987generating,lugea2023stylistics}.
Combining multiple HLFs that are revelatory of linguistic style could provide us
with an easily interpretable and verifiable stylistic profile.

These are the strands of thought that have inspired us to pursue two research
questions in the age of LLMs.

Firstly, \textbf{is it possible to influence an LLM's writing style by
    instructing it to generate text that complies to certain HLFs}?
This problem of controlled text generation has implications for text style
transfer and authorship verification.
Answering this question might point out cheaper, less intrusive and more user
friendly solutions than fine tuning large language models for these tasks.
Fine tuning LLMs also incurs some loss of generality~\cite{yang2024unveiling},
going directly against the purpose of having a generic language model.

Secondly, we ask \textbf{if it's possible to quantify how closely an LLM is able
    to follow prompts concerning the compliance of the generated text to HLFs?}
The answer to this question could hint at a relatively simple and accurate way
of evaluating the quality of (partial) text style transfer using a large
language model.

\section{Related Work}\label{related}

There is abundent literature on the topic of text style transfer.
Reviews and surveys~\cite{tst-review-2021,tst-survey-2022} were helpful for
informing our selection of HLFs.
Studies on attaining fine-grained text style transfer using language models
have served as inspiration for writing this paper~\cite{lyu-etal-2023-fine}.
We should note though that we are not attempting to transfer text style, but
merely to instruct an LLM to follow instructions based on HLFs.

Linguistic style is the sum of identifiable language choices manifest in a text
made from the language system by the text producer~\cite{lugea2023stylistics}.
Another way to think about it is that style is the form used for delivering
meaning~\cite{tst_sigkdd_review_2022}.
The ample literature on the subject seems to agree that style depends on context
and author choice vis-a-vis a communication
goal~\cite{mcdonald1985computational,hovy1987generating}.
The author's choice is available at all linguistic levels, including the
morphological, lexical, syntactic, semantic and pragmatic
levels~\cite{dimarco1994model,lugea2023stylistics}.
The influence of syntactic structure over text style is apparent also from a
formalist perspective~\cite{chomsky2002syntactic}.
This work grounds our choice of handcrafted linguistic features related to
linguistic style.

The idea of constructing style from fine-grained aspects is not new and there
are excellent studies on the subject.
The StylePTB authors explain how this dataset leverages lexical, syntactic,
semantic and thematic aspects to allow verifying text style transfer more
thoroughly than previous methods~\cite{lyu-etal-2021-styleptb}.
Besides providing the understanding required to distinguish between semantic
transfer and other forms of style transfer, the dataset itself is useful for
evaluating HLFs that function at the sentence level.

The body of work on HLFs is extensive.
It is well referenced and synthesised by Lee and Lee~\cite{lftk-2023}.
To our knowledge there is no work that connects HLFs with LLMs in the manner
described in this paper.
HLFs are used in the context of other, connected tasks such as assessing text
readability~\cite{lee-etal-2021-pushing}.

In terms of controlling the style of the generated text, our approach differs
from others in at least two significant ways: simplicity and focus.
We are constructing well crafted user prompts for an LLM instead of diving into
more complex artificial intelligence solutions.
Our focus is on controlling text generation using HLFs and measuring the change
relative to a baseline obtained without using HLFs in the generation process.
We are not discussing the effectiveness of style transfer, author verification
or other tasks (yet).

Our paper describes an experiment in controllable text generation
(CTG)~\cite{zhang-ctg-2022}.
Hightened recent interest in CTG and especially in benchmarking LLMs in the
context of CTG~\cite{chen2024benchmarking} is particularly relevant for this
paper.
Such research alleviates the burden of proof regarding the level to which
LLMs respond well to diversified instructions for controlled text generation in
general.
This paper differs from the existing CTG work with its use of HLFs for writing
the text generation instructions.
It additionally uses HLFs to measure the performance of LLMs in terms of their
ability to generate text that conforms to some chosen target HLF values.

\section{Experiment Design}\label{method}

Our experiment analyses LLM output when asked to generate new text based on
given input text and instructions derived from HLFs.
We compute statistics of HLFs on a corpus to derive the text generation
instructions.

To run the experiment we must select the LLMs to evaluate and finding datasets
from which we can derive corpuses.
Then there's the matter of selecting the HLFs for our experiment.

We think it's best to describe the experimental process before going over our
selection for the experiment's parameters.

\subsection{Process}\label{subsec:experiment-process}

Assume we have a target LLM, a corpus containing texts, some examples extracted
from the corpus, a selection of HLFs and input text both from within and from
outside the corpus.
This description of the experiment process assumes those parameters have stable
values.
Changing the parameters requires restarting the process.

The first activity involves establishing baselines for each experiment scenario.

In the first scenario the target LLM is instructed to generate text retaining
the meaning of  the input text without any further instruction related to style
or the linguistic features of the generated text.
This step is repeated 10 times, computing each HLF in our selection for the
generated text.
The computed values make up the first scenario baselines for each HLF.

The second scenario consists in generating text from the same input text, but
with the aid of some examples from the corpus.
In this scenario, we believe that the choice of input text affects the outcome.
Therefore we propose two variants for this scenario: one with input text
extraneous to the corpus and one with input text from the corpus.

We add the examples to the initial LLM prompt and follow the same process like
in the first scenario.
We do this for each variant of the second scenario to obtain two more baselines
for each HLF for the corpus.

The number of examples is chosen so that we do not overrun the context window of
the LLM.
Some context windows are smaller than others, settling the number of examples at
three.

The next step requires computing statistics on the corpus for each HLF:
\begin{itemize}
    \item \textbf{min}: the minimum value of the HLF on the corpus;
    \item \textbf{max}: the maximum value of the HLF on the corpus;
    \item \textbf{avg}: the mean value of the HLF on the corpus.
\end{itemize}

Doing so allows writing an HLF prompt derived from each of our baseline prompts.
The HLF prompts instruct the LLM to generate text whose HLFs comply with the
previously computed statistics.
The individual (per-HLF) instructions are all are part of the same HLF prompt.
The prompts are constructed using Jinja templates which are located in the
\texttt{\textlinkanon{https://github.com/koosie0507/llm-tst-consistency/tree/main/src/llm_tst_consistency/templates}{templates}}
directory of the source code used to run the experiment.
The instruction prompts corresponding to our two scenarios are
`\texttt{prompt\_1}' and `\texttt{prompt\_2}', respectively.
The rest of the templates in the directory are used to construct the individual
HLF instructions.

In a similar manner to how we generated the baselines, we run 10 trials for each
scenario and variant.
We compare each baseline with the results obtained by using the corresponding
HLF prompt.

The first scenario is indicative of whether LLMs acknowledge HLF instructions.
If they do then the results obtained from HLF instructions should be
significantly different from the corresponding baseline results.

Both variants of the second scenario quantify how similar the HLFs of the texts
generated by the LLM are to the corpus average \textit{because of} the HLF
instructions.
The rationale here is that the LLM can generate text very similar to text from
the corpus by using the supplied examples from the corpus.
That would leave the HLF instructions as the remaining possible cause for
generating text with HLFs that are consistently different from the corresponding
baseline.

\subsection{LLM Selection}\label{llm-selection}

We only use state of the art large language models for the experiment.
The Chatbot Arena~\cite{chiang2024chatbot} leaderboard is essential in our
selection.
To cover more ground we find top performers from different vendors interesting.
The final selection is available in Table~\ref{table-llm}.

\begin{table*}[ht]
    \setlength\tabcolsep{6pt}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llllll@{}}
            \toprule
            ID          & Name                    & Parameters  & Context Size & Elo Score & License Type        \\ \toprule
            gpt         & OpenAI GPT-4o           & undisclosed & 128000       & 1287      & Proprietary         \\
            gemini      & Google Gemini 1.5 Pro   & undisclosed & 1048576      & 1268      & Proprietary         \\
            claude3     & Anthropic Claude 3 Opus & undisclosed & 200000       & 1248      & Proprietary         \\
            llama3\_70b & Meta Llama 3-70B        & 70 Billion  & 8000         & 1208      & Llama 3 Community   \\
            command\_r  & Cohere Command R+       & 104 Billion & 128000       & 1189      & CC-BY-NC 4.0        \\
                        &                         &             &              &           & with Acceptable Use \\
                        &                         &             &              &           & Addendum            \\ \bottomrule
        \end{tabular}%
    }
    \caption{Large Language Model Selection}\label{table-llm}
\end{table*}

The ID of the LLM is referenced throughout figures, tables and generally
throughout the rest of the paper.

\subsection{HLF Selection}\label{hlf-selection}

Our main focus is to understand how susceptible LLMs are to prompts containing
instructions to conform to HLFs.
LFTK authors~\cite{lftk-2023} categorize HLFs by domain and family and make
available information on the suitability of each HLF for given tasks such as
readability assessment or fake news detection.
Besides the domain and family, we also choose features based on their influence
over text style as recognized in literature~\cite{verma2019lexical,lugea2023stylistics}.

The instructions for each HLF are specifically crafted using natural language.
The resulting instructions can be catalogued on the concrete-abstract spectrum.
For example, `write 100 words' is a more concrete instruction than `write as if
you were in junior highschool'.
We divide HLFs in five categories ranging from very concrete to highly abstract
and choose 2 features from each category.
Table~\ref{table-hlf} showcases our selection of features.

\begin{table*}[ht]
    \setlength\tabcolsep{6pt}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llllll@{}}
            \toprule
            LFTK ID         & Name                              & Family           & Domain           & Abstraction     & Style Level\footnotemark \\ \toprule
            t\_word         & total words                       & wordsent         & surface          & very concrete   & lexical                  \\
            t\_sent         & total sentences                   & wordsent         & surface          & very concrete   & syntactic                \\ \bottomrule
            n\_uverb        & total number of unique adjectives & partofspeech     & syntax           & concrete        & semantic                 \\
            n\_uadj         & total number of unique adjectives & partofspeech     & syntax           & concrete        & syntactic                \\ \bottomrule
            simp\_ttr       & simple type token ratio           & typetokenratio   & lexico-semantics & regular         & pragmatic                \\
            a\_verb\_pw     & number of verbs per word          & partofspeech     & surface          & regular         & syntactic                \\ \bottomrule
            corr\_adj\_var  & corrected adjective variation     & lexicalvariation & lexico-semantics & abstract        & lexical                  \\
            corr\_verb\_var & corrected verb variation          & lexicalvariation & lexico-semantics & abstract        & lexical                  \\ \bottomrule
            fkgl            & Flesch-Kincaid grade level        & readformula      & surface          & highly abstract & pragmatic                \\
            a\_kup\_pw      & Kuperman age of aquisition        & worddiff         & lexico-semantics & highly abstract & lexical                  \\ \bottomrule
        \end{tabular}%
    }
    \caption{Handcrafted Linguistic Features}\label{table-hlf}
\end{table*}
\footnotetext{as described in the figure in~\cite{lugea2023stylistics}, chapter 1, page 7}

The LFTK ID is used throughout tables and figures to refer to a specific HLF.

\subsection{Corpus Selection}\label{ds-selection}

The experiment design rests upon measuring HLFs on a corpus of texts and
computing statistics.
LLMs also make use of examples extracted from a corpus to guide text generation.
Our approach idempotently derives a corpus of texts from a well known dataset.
Our selection of datasets is listed in Table~\ref{table-ds}.

\begin{table*}[ht]
    \setlength\tabcolsep{6pt}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}lllll@{}}
            \toprule
            Name            & Task                     & Number of Samples & Content Type & Reference              \\ \toprule
            Yelp Reviews    & Sentiment Classification & 50000             & Reviews      & \cite{yelp2015neurips} \\
            (test split)    & Text Style Transfer      &                   &              &                        \\
                            & Text Classification      &                   &              &                        \\ \midrule
            CNN / DailyMail & Summarization            & 11490             & News         & \cite{cnndm2015}       \\
            (test split)    & Question Answering       &                   & Stories      & \cite{cnndm2017}       \\
                            & Text Generation          &                   &              &                        \\ \midrule
        \end{tabular}%
    }
    \caption{Source Datasets}\label{table-ds}
\end{table*}

We use the Huggingface ecosystem~\cite{lhoest-etal-2021-datasets} to load the
datasets.
Yelp reviews are loaded from \texttt{Yelp/yelp\_review\_full}, while CNN stories
are loaded from \texttt{abisee/cnn\_dailymail}.

The Yelp review data set provides a diverse assortment of reviews from online
users.
The authors employ a casual writing style, very similar to the writing style of
an average person.
We construct a corpus containing the first 1000 reviews from the test split of
this dataset.

CNN/DailyMail contains stories and news with diverse styles.
The style varies from one author to another and it also follows the publisher's
editorial style.
The articles are more formal, longer and more complex in structure than Yelp
reviews which makes this dataset a good alternative.
The corpus that is based on this dataset contains the first 1000 CNN articles
from the test split of the dataset.

\subsection{User Prompt and Text}\label{input-text}

This experiment presupposes instructing various large language models to reword
an input text so that it conforms to some HLFs.

There is an input text corresponding to each corpus that is external to that
corpus.
These texts are located in the
\texttt{\textlinkanon{https://github.com/koosie0507/llm-tst-consistency/tree/main/data}{data}}
folder of the experiment's source code.

For the variant that uses an input text from the corpus, we draw the text
randomly from the corpus once, at the start of the experiment.

The input text that is external to the CNN article corpus is an extract from the
classic \textit{The Life and Opinions of Tristram Shandy, Gentleman}, by
Laurence Sterne~\cite{sterne2003life}.
The text style is very dissimilar compared to news articles.
Its linguistic features are also very unlike the average on the CNN article
corpus.
A text that is similarly divorced from the style and linguistic features of most
Yelp reviews is Oscar Wilde's \textit{Sonnet toLiberty}~\cite{wilde1909poems}.

Choosing input texts that are vastly different from their corresponding corpus
is based in the assumption that LLMs would generate baseline results that do not
conform to the HLF statistics on that corpus.

\section{Experiment Results}

\subsection{Interpretation Conventions}

Handcrafted linguistic features are integer or floating point values that we
can compute on text.
Plot figures show the HLF on the vertical axis, while the horizontal axis is
reserved for the trial number.
HLF values for the text generated using baseline prompts are plotted as a
continuous grey line, while the values obtained by using HLF enhanced prompts
are plotted using a continuous grenat line.
The minimum and maximum HLF on the benchmark dataset are plotted using dotted
black lines.
A continuous black line designates the average HLF on the corpus.
We sometimes refer to this average as the \textit{target} value because the LLM
is instructed to generate text that primarily conforms to this value.

Result tables highlight the difference between plots obtained using a
baseline prompt versus the ones obtained using HLF instruction prompts.
Firstly, we measure how different baselines are from HLF-enhanced plots.
Secondly, we compare the closeness of the baseline and HLF-enhanced plots to the
target average.
Table values where the text is \textbf{bold} mark significant differences
between baseline and HLF instruction results.
\underline{Underlined} values mark situations where the result of using HLF
instructions is closer to the target than the baseline.
\textit{Italic} values mark situations where the baseline is closer to the
target.

\subsection{Measurements}\label{subsec:measurements}

We use the two-sampled Kolmogorov-Smirnov~\cite{kolmogorov1933,smirnov1939} test
to determine whether the baseline results are different from the HLF-enhanced
results.
We consider that the results are different for a resulting $p\_value \le 0.05$.

To determine which of two results is closer to the target, we compute the area
between each result and the target.
Another way in which we measure the closeness of the result to the target is by
computing the Euclidian norm of the vectorial difference between the result and
target.

Note that each HLF can have a different range of values than other HLFs.
When we compute the area and the Euclidian norm, we use the raw values of each
HLF, without preprocessing them.
Therefore the results for one HLF are not comparable with the results for
another HLF.

Remember that we don't quantify how different the baseline is from the
HLF-enhanced result.
Instead we calculate the relative closeness to a common target.
Because we took this approach, comparing values for the same HLF, but different
LLMs is also ill-advised.

Let us denote with $A_b$ and $A_h$ the areas between baseline and target and HLF
instruction result and target, respectively.
Let us similarly denote with $N_b$ and $N_h$ the Euclidian norms.
We define $Diff_A = A_b - A_h$ and $Diff_N = N_b - N_h$.

The result tables show $Diff_A$ and $Diff_N$ for each LLM in the context of a
prompt and a dataset.
All result tables for each dataset and prompt variant are available in the
appendix.

In the next sections we present our perspective on the most relevant results of
the experiment.

\subsection{HLF Instruction Impact}

The results on the CNN article corpus showcased in Table~\ref{table-prompt-1-cnn-dailymail}
shows that LLMs take into account instructions concerning the HLFs of their
output.

The results on the review corpus generated from the Yelp dataset are similarly
significant, but the HLF instructions determine outcomes that are further away
from the target.

\begin{table*}[htp]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word                      & t\_sent                    & n\_uverb                    & n\_uadj                    & simp\_ttr                 & a\_verb\_pw               & corr\_adj\_var            & corr\_verb\_var            & fkgl                    & a\_kup\_pw                \\
            \toprule
            claude3 $Diff\_N$     & \textbf{\underline{623.16}}  & \textbf{\underline{24.74}} & \textbf{\underline{51.18}}  & \textbf{\underline{26.68}} & 0.06                      & -0.0                      & \textbf{\underline{0.46}} & \textbf{\underline{1.4}}   & 1.44                    & \textbf{\textit{-1.66}}   \\ \midrule
            claude3 $Diff\_A$     & \textbf{\underline{1774.0}}  & \textbf{\underline{74.0}}  & \textbf{\underline{158.84}} & \textbf{\underline{81.2}}  & 0.2                       & -0.01                     & \textbf{\underline{1.88}} & \textbf{\underline{4.49}}  & 5.01                    & \textbf{\textit{-4.09}}   \\ \midrule
            gemini $Diff\_N$      & \textbf{\underline{985.86}}  & \textbf{\underline{34.52}} & \textbf{\underline{100.36}} & \textbf{\underline{53.39}} & \textbf{\underline{0.52}} & 0.05                      & \textbf{\underline{2.62}} & \textbf{\underline{4.68}}  & -4.17                   & \textbf{\textit{-2.43}}   \\ \midrule
            gemini $Diff\_A$      & \textbf{\underline{2990.5}}  & \textbf{\underline{106.5}} & \textbf{\underline{320.34}} & \textbf{\underline{159.1}} & \textbf{\underline{1.5}}  & 0.09                      & \textbf{\underline{7.24}} & \textbf{\underline{13.15}} & -12.65                  & \textbf{\textit{-7.13}}   \\ \midrule
            gpt $Diff\_N$         & \textbf{\underline{125.87}}  & 1.46                       & 10.95                       & 10.77                      & 0.02                      & -0.01                     & 0.71                      & 0.81                       & \textbf{\textit{-1.72}} & 0.43                      \\ \midrule
            gpt $Diff\_A$         & \textbf{\underline{327.0}}   & 3.0                        & 29.0                        & 27.0                       & 0.08                      & -0.01                     & 1.82                      & 2.24                       & \textbf{\textit{-5.72}} & 0.44                      \\ \midrule
            command\_r $Diff\_N$  & \textbf{\underline{1024.78}} & \textbf{\underline{39.67}} & \textbf{\underline{81.58}}  & \textbf{\underline{41.46}} & \textbf{\underline{0.46}} & \textbf{\underline{0.03}} & \textbf{\underline{2.67}} & \textbf{\underline{3.78}}  & 1.4                     & \textbf{\underline{0.14}} \\ \midrule
            command\_r $Diff\_A$  & \textbf{\underline{3132.5}}  & \textbf{\underline{120.5}} & \textbf{\underline{246.84}} & \textbf{\underline{132.2}} & \textbf{\underline{1.36}} & \textbf{\underline{0.06}} & \textbf{\underline{7.88}} & \textbf{\underline{11.21}} & 3.9                     & \textbf{\underline{1.03}} \\ \midrule
            llama3\_70b $Diff\_N$ & \textbf{\underline{459.45}}  & \textbf{\underline{17.78}} & \textbf{\underline{36.5}}   & \textbf{\underline{19.75}} & \textbf{\underline{0.12}} & \textbf{\underline{0.03}} & \textbf{\underline{1.43}} & \textbf{\underline{2.18}}  & 1.5                     & \textbf{\textit{-0.06}}   \\ \midrule
            llama3\_70b $Diff\_A$ & \textbf{\underline{1363.5}}  & \textbf{\underline{54.0}}  & \textbf{\underline{106.5}}  & \textbf{\underline{60.5}}  & \textbf{\underline{0.36}} & \textbf{\underline{0.09}} & \textbf{\underline{4.33}} & \textbf{\underline{6.38}}  & 7.46                    & \textbf{\textit{-0.11}}   \\ \bottomrule
        \end{tabular}%
    }\caption{CNN Results}\label{table-prompt-1-cnn-dailymail}
\end{table*}

Figure~\ref{fig-command-r-t-word} shows a positive result, where the result
obtained with the aid of HLF instructions is closer to the target.
Figure~\ref{fig-gpt-fkgl} shows a negative result, where the baseline is closer
to the target.
Figure~\ref{fig-llama-simp-ttr} shows an inconclusive result.

\begin{figure*}[htp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_1/prompt_1-command_r-cnn_dailymail/prompt_1-command_r-cnn_dailymail_t_word.png}
        \caption{Command-R+\\Total Words}
        \label{fig-command-r-t-word}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_1/prompt_1-gpt-cnn_dailymail/prompt_1-gpt-cnn_dailymail_fkgl.png}
        \caption{GPT\\Flesch-Kincaid Grade Level}
        \label{fig-gpt-fkgl}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_1/prompt_1-claude3-cnn_dailymail/prompt_1-claude3-cnn_dailymail_a_verb_pw.png}
        \caption[center]{Claude3\\Average Verbs per Word}
        \label{fig-llama-simp-ttr}
    \end{minipage}
\end{figure*}

\subsection{HLF Instructions and Context Awareness}

In the second scenario of the experiment we try to quantify the impact of using
HLF instructions on the output of the LLM.
There are two variants of this scenario: input text from within or from outside
the corpus.

Table~\ref{table-prompt-2-cnn-dailymail} contains results for the first variant.
The outlook is similar to the first experiment scenario.

\begin{table*}[hpb]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word                      & t\_sent                    & n\_uverb                    & n\_uadj                    & simp\_ttr                 & a\_verb\_pw               & corr\_adj\_var            & corr\_verb\_var           & fkgl                       & a\_kup\_pw                \\
            \toprule
            claude3 $Diff\_N$     & \textbf{\underline{718.49}}  & \textbf{\underline{24.13}} & \textbf{\underline{44.43}}  & \textbf{\underline{23.43}} & \textbf{\underline{0.4}}  & 0.03                      & 0.94                      & \textbf{\underline{0.77}} & 1.2                        & 0.07                      \\ \midrule
            claude3 $Diff\_A$     & \textbf{\underline{2345.54}} & \textbf{\underline{90.18}} & \textbf{\underline{148.36}} & \textbf{\underline{63.6}}  & \textbf{\underline{1.18}} & 0.07                      & 1.36                      & \textbf{\underline{2.55}} & 3.33                       & 0.52                      \\ \midrule
            gemini $Diff\_N$      & \textbf{\underline{785.6}}   & \textbf{\underline{25.8}}  & \textbf{\underline{60.5}}   & \textbf{\underline{41.6}}  & \textbf{\underline{0.41}} & \textbf{\underline{0.05}} & \textbf{\underline{1.95}} & \textbf{\underline{2.67}} & -4.62                      & \textbf{\textit{-1.94}}   \\ \midrule
            gemini $Diff\_A$      & \textbf{\underline{2512.0}}  & \textbf{\underline{81.5}}  & \textbf{\underline{210.34}} & \textbf{\underline{125.7}} & \textbf{\underline{1.25}} & \textbf{\underline{0.09}} & \textbf{\underline{5.84}} & \textbf{\underline{8.05}} & -9.46                      & \textbf{\textit{-5.82}}   \\ \midrule
            gpt $Diff\_N$         & 47.8                         & 5.18                       & 6.13                        & -0.13                      & 0.02                      & 0.0                       & 0.05                      & 0.3                       & 3.06                       & -0.27                     \\ \midrule
            gpt $Diff\_A$         & 77.0                         & 12.5                       & 13.0                        & -4.0                       & 0.04                      & 0.0                       & -0.17                     & 0.77                      & 8.45                       & -0.16                     \\ \midrule
            command\_r $Diff\_N$  & \textbf{\underline{263.51}}  & 5.35                       & \textbf{\underline{25.08}}  & \textbf{\underline{17.37}} & \textbf{\textit{-0.09}}   & -0.01                     & \textbf{\underline{1.74}} & \textbf{\underline{1.42}} & \textbf{\underline{10.46}} & \textbf{\underline{1.56}} \\ \midrule
            command\_r $Diff\_A$  & \textbf{\underline{896.0}}   & 20.5                       & \textbf{\underline{86.0}}   & \textbf{\underline{60.6}}  & \textbf{\textit{-0.21}}   & -0.03                     & \textbf{\underline{5.45}} & \textbf{\underline{4.95}} & \textbf{\underline{31.29}} & \textbf{\underline{5.67}} \\ \midrule
            llama3\_70b $Diff\_N$ & \textbf{\underline{376.0}}   & \textbf{\underline{15.63}} & \textbf{\underline{38.63}}  & 15.74                      & \textbf{\underline{0.24}} & \textbf{\underline{0.06}} & 1.69                      & \textbf{\underline{2.91}} & 0.31                       & -0.3                      \\ \midrule
            llama3\_70b $Diff\_A$ & \textbf{\underline{1088.5}}  & \textbf{\underline{45.0}}  & \textbf{\underline{113.0}}  & 41.0                       & \textbf{\underline{0.65}} & \textbf{\underline{0.18}} & 3.53                      & \textbf{\underline{8.56}} & 0.27                       & -0.68                     \\ \bottomrule
        \end{tabular}%
    }
    \caption{prompt 2 - CNN Results\\Input Outside Corpus}
    \label{table-prompt-2-cnn-dailymail}
\end{table*}

A possible explanation for having fewer significant differences in results might
lie in the closeness to the target.
As we can see in Figure~\ref{fig-p2-gpt-twords}, this is not always the case.
In~\cref{fig-p2-command-r-simpttr,fig-p2-gemini-a-kup-pw} we observe regressions
in performance with significant differences between baseline and result.

\begin{figure*}[hpb]
    \centering
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2/prompt_2-gpt-cnn_dailymail/prompt_2-gpt-cnn_dailymail_t_word.png}
        \caption{GPT\\Total Words}
        \label{fig-p2-gpt-twords}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2/prompt_2-command_r-cnn_dailymail/prompt_2-command_r-cnn_dailymail_simp_ttr.png}
        \caption{Command-R+\\Simple TTR}
        \label{fig-p2-command-r-simpttr}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2/prompt_2-gemini-cnn_dailymail/prompt_2-gemini-cnn_dailymail_a_kup_pw.png}
        \caption[center]{Gemini\\Kuperman Age of Aquisition}
        \label{fig-p2-gemini-a-kup-pw}
    \end{minipage}
\end{figure*}

Even so, the results obtained in our first scenario are largely confirmed in the
first variant of our second scenario.

We finally look at the second variant of the second scenario of our experiment.
The hypothesis in this variant is that LLMs will generate text that exhibits
HLFs much closer to the target corpus average if the input text is from that
corpus.
The expectation is to see less significant results.

\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word                      & t\_sent                     & n\_uverb                    & n\_uadj                    & simp\_ttr                 & a\_verb\_pw               & corr\_adj\_var             & corr\_verb\_var            & fkgl                       & a\_kup\_pw              \\
            \toprule
            claude3 $Diff\_N$     & \textbf{\underline{871.02}}  & \textbf{\underline{34.2}}   & \textbf{\underline{34.48}}  & \textbf{\textit{-7.38}}    & \textbf{\underline{0.27}} & 0.01                      & \textbf{\textit{-0.93}}    & \textbf{\textit{-0.72}}    & 1.16                       & -0.14                   \\ \midrule
            claude3 $Diff\_A$     & \textbf{\underline{2756.12}} & \textbf{\underline{117.18}} & \textbf{\underline{126.36}} & \textbf{\textit{-11.2}}    & \textbf{\underline{0.81}} & -0.01                     & \textbf{\textit{-2.52}}    & \textbf{\textit{-0.74}}    & 3.69                       & -0.4                    \\ \midrule
            gemini $Diff\_N$      & \textbf{\underline{1315.41}} & \textbf{\underline{46.19}}  & \textbf{\underline{114.47}} & \textbf{\underline{72.38}} & \textbf{\underline{1.17}} & 0.03                      & \textbf{\underline{5.25}}  & \textbf{\underline{6.64}}  & -1.6                       & \textbf{\textit{-2.1}}  \\ \midrule
            gemini $Diff\_A$      & \textbf{\underline{3996.0}}  & \textbf{\underline{138.0}}  & \textbf{\underline{357.36}} & \textbf{\underline{212.7}} & \textbf{\underline{3.3}}  & 0.06                      & \textbf{\underline{15.49}} & \textbf{\underline{19.4}}  & -13.22                     & \textbf{\textit{-6.19}} \\ \midrule
            gpt $Diff\_N$         & \textbf{\underline{521.29}}  & \textbf{\underline{23.3}}   & \textbf{\underline{43.13}}  & 19.85                      & \textbf{\underline{0.2}}  & 0.02                      & 1.08                       & \textbf{\underline{1.04}}  & 5.09                       & 0.67                    \\ \midrule
            gpt $Diff\_A$         & \textbf{\underline{1464.0}}  & \textbf{\underline{65.5}}   & \textbf{\underline{121.84}} & 58.1                       & \textbf{\underline{0.58}} & 0.05                      & 2.93                       & \textbf{\underline{2.59}}  & 14.23                      & 1.89                    \\ \midrule
            command\_r $Diff\_N$  & \textbf{\underline{371.08}}  & \textbf{\underline{13.31}}  & 22.06                       & \textbf{\underline{17.19}} & \textbf{\underline{0.17}} & \textbf{\underline{0.02}} & 0.88                       & 1.09                       & \textbf{\underline{4.05}}  & -0.15                   \\ \midrule
            command\_r $Diff\_A$  & \textbf{\underline{1198.0}}  & \textbf{\underline{42.5}}   & 73.0                        & \textbf{\underline{56.5}}  & \textbf{\underline{0.69}} & \textbf{\underline{0.07}} & 2.85                       & 3.99                       & \textbf{\underline{13.89}} & -0.11                   \\ \midrule
            llama3\_70b $Diff\_N$ & \textbf{\underline{1040.85}} & \textbf{\underline{41.99}}  & \textbf{\underline{80.58}}  & \textbf{\underline{49.12}} & \textbf{\underline{0.55}} & \textbf{\underline{0.04}} & \textbf{\underline{3.99}}  & \textbf{\underline{5.05}}  & 7.18                       & \textbf{\textit{-1.94}} \\ \midrule
            llama3\_70b $Diff\_A$ & \textbf{\underline{2928.0}}  & \textbf{\underline{118.5}}  & \textbf{\underline{222.0}}  & \textbf{\underline{133.5}} & \textbf{\underline{1.43}} & \textbf{\underline{0.05}} & \textbf{\underline{10.77}} & \textbf{\underline{13.42}} & 17.21                      & \textbf{\textit{-6.2}}  \\ \bottomrule
        \end{tabular}%
    }
    \caption{CNN Results\\Input Inside Corpus}
    \label{table-prompt-2-ifd-cnn-dailymail}
\end{table*}

The results shown in Table~\ref{table-prompt-2-ifd-cnn-dailymail} contradict our
expectations outright.
The most surprising behaviour is exhibited for HLFs which are derived into more
concrete generation instructions, such as the total number of words.
This behaviour is highlighted in figure~\ref{fig-p2-ifd-gemini-twords}.
Additionally, we notice regressive behaviour for some models compared to the
previously explored results in figure~\ref{fig-p2-ifd-claude3-nuadj}.
Finally, the abstractness of the text generation instructions for certain HLFs
leads to the generated text not being compliant as exemplified in
figure~\ref{fig-p2-ifd-llama3-a-kup-pw}.

\begin{figure*}[ht]
    \centering
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2_ifd/prompt_2-claude3-cnn_dailymail/prompt_2-claude3-cnn_dailymail_n_uadj.png}
        \caption{Claude3\\Number of Unique Adjectives}
        \label{fig-p2-ifd-claude3-nuadj}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2_ifd/prompt_2-gemini-cnn_dailymail/prompt_2-gemini-cnn_dailymail_t_word.png}
        \caption{Gemini\\Total Words}
        \label{fig-p2-ifd-gemini-twords}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \includegraphics[width=\linewidth]{plots/prompt_2_ifd/prompt_2-llama3_70b-cnn_dailymail/prompt_2-llama3_70b-cnn_dailymail_a_kup_pw.png}
        \caption[center]{Llama3\\Kuperman Age of Aquisition}
        \label{fig-p2-ifd-llama3-a-kup-pw}
    \end{minipage}
\end{figure*}

The results in this last scenario show that whether LLMs take examples listed in
the prompt into account is debatable when it comes to generating text with
similar linguistic features as the examples'.

\section{Discussion}

In terms of our first question --- whether LLMs are able to follow instructions
concerning handcrafted linguistic features --- the response is in our opinion
affirmative.
Certainly, not all LLMs are created equal in this regard.

The design of the experiment leaves room for improvement because of the choices
regarding computing HLF statistics on a corpus, input text, the corpus
itself or the instruction prompt.
The importance of these factors is evidentiated by the difference in results
obtained using the Yelp reviews corpus when compared to the results obtained
on the CNN article corpus.
While these results also show a significant influence when using HLFs in the
prompt, the influence is mostly negative.

This leads us to our second question: how much do handcrafted linguistic feature
instructions affect the way LLMs write text.
The results in this regards show us a much less conclusive picture.
It is not apparent the LLM will follow HLF instructions as intended.
In fact, using the same prompt on different data lead to wildly different
outcomes.
Whether this is because of circumstances such as the selection of HLFs, the LLM,
the HLF target values, the input text, a.s.o remains to be determined.
We can observe some idioms, though.

The more abstract the instruction to comply to an HLF, the fewer the chances of
a significant impact on the outcome.
When the outcome is significant, it tends to be negative.
This tells us that large language models are likely good at following
instructions about HLFs when those instructions are more concrete.

Then, the choice of input text in relation to the target HLF values seems very
important.
LLMs don't seem able yet to cover the vast gulf between the linguistic features
of an Oscar Wilde poem and the average linguistic features of text written by
Yelp reviewers.
This leads us to propose that even if LLMs are able to generate text that
complies to certain target HLFs, there is a limit to this ability.

Finally, the overall selection of HLFs is important.
Choosing HLFs that measure the uniqueness of a linguistic element (words,
structures, a.s.o) limits the LLMs ability to conform to other HLFs (for example
total number of sentences or reading ease).
This is highlighted in the experiment specifically in the tension between the
age of acquisition and the adjective or verb variation.

\section{Conclusions and Future Work}

We started out by trying to understand whether it is possible to generate text
that exhibits certain linguistic features using a large language model.
To our surprise, it turns out that instructing language models to write text
that conforms to a selected assortment of linguistic features almost always
leads to significantly different linguistic featues of generated text.

The results are not always satisfying, though.
At this stage, the more pragmatic answer seems to involve prompt engineering and
a careful choice of language features.
Providing examples does not seem to influence the model's ability to comply with
the user's desires.

Other experiments are required to better understand the capabilities of large
language models with regard to handcrafted linguistic features.
For example, testing the response of language models to features in isolation
or not using a corpus or input text are different setups that could eliminate
some of the unknowns left uncovered by the current experiment.

\bibliography{references}

\appendix

\section{Additional Result Tables}\label{sec:yelp-tables}
\subsection{Yelp}
\Cref{table-prompt-1-yelp,table-prompt-2-yelp,table-prompt-2-ifd-yelp} show the
experiment results on the corpus of reviews extracted from the Yelp dataset.

\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word                   & t\_sent                    & n\_uverb                 & n\_uadj                  & simp\_ttr & a\_verb\_pw             & corr\_adj\_var          & corr\_verb\_var         & fkgl                       & a\_kup\_pw               \\
            \toprule
            claude3 $Diff\_N$     & \textbf{\textit{-281.9}}  & \textbf{\textit{-13.22}}   & \textbf{\textit{-25.79}} & -14.56                   & -0.06     & 0.0                     & -1.27                   & -1.14                   & \textbf{\underline{6.89}}  & \textbf{\underline{1.0}} \\ \midrule
            claude3 $Diff\_A$     & \textbf{\textit{-774.0}}  & \textbf{\textit{-27.76}}   & \textbf{\textit{-70.0}}  & -38.0                    & -0.19     & 0.02                    & -3.25                   & -3.39                   & \textbf{\underline{21.68}} & \textbf{\underline{2.9}} \\ \midrule
            gemini $Diff\_N$      & -109.27                   & 0.05                       & -7.25                    & -13.18                   & -0.14     & 0.01                    & -1.01                   & -0.4                    & 1.62                       & -0.4                     \\ \midrule
            gemini $Diff\_A$      & -252.82                   & -0.78                      & -17.5                    & -23.14                   & -0.52     & 0.01                    & -1.85                   & -1.07                   & 7.14                       & -0.93                    \\ \midrule
            gpt $Diff\_N$         & -21.62                    & 2.42                       & -3.8                     & -8.84                    & 0.0       & -0.01                   & -0.8                    & -0.53                   & 8.94                       & -0.37                    \\ \midrule
            gpt $Diff\_A$         & -34.72                    & 9.68                       & -10.54                   & -24.57                   & -0.03     & -0.02                   & -2.37                   & -1.58                   & 19.26                      & -1.03                    \\ \midrule
            command\_r $Diff\_N$  & -40.1                     & 1.22                       & 2.43                     & \textbf{\textit{-13.64}} & 0.05      & \textbf{\textit{-0.04}} & \textbf{\textit{-1.17}} & 0.59                    & \textbf{\textit{-7.25}}    & \textbf{\textit{-1.67}}  \\ \midrule
            command\_r $Diff\_A$  & -75.5                     & 1.5                        & 12.94                    & \textbf{\textit{-34.0}}  & 0.19      & \textbf{\textit{-0.12}} & \textbf{\textit{-2.76}} & 2.83                    & \textbf{\textit{-20.49}}   & \textbf{\textit{-4.76}}  \\ \midrule
            llama3\_70b $Diff\_N$ & \textbf{\textit{-210.45}} & \textbf{\underline{6.87}}  & \textbf{\textit{-13.46}} & \textbf{\textit{-15.17}} & -0.16     & \textbf{\textit{-0.02}} & \textbf{\textit{-1.78}} & \textbf{\textit{-1.31}} & 3.7                        & \textbf{\textit{-1.58}}  \\ \midrule
            llama3\_70b $Diff\_A$ & \textbf{\textit{-607.52}} & \textbf{\underline{19.42}} & \textbf{\textit{-41.22}} & \textbf{\textit{-44.77}} & -0.42     & \textbf{\textit{-0.05}} & \textbf{\textit{-5.48}} & \textbf{\textit{-4.34}} & 9.79                       & \textbf{\textit{-4.98}}  \\ \bottomrule
        \end{tabular}%
    }
    \caption{Yelp Results - Prompt Without Examples}
    \label{table-prompt-1-yelp}
\end{table*}

\begin{table*}[!ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word & t\_sent & n\_uverb & n\_uadj & simp\_ttr & a\_verb\_pw & corr\_adj\_var            & corr\_verb\_var & fkgl                       & a\_kup\_pw \\
            \toprule
            claude3 $Diff\_N$     & -121.77 & -4.61   & -9.9     & 18.41   & -0.11     & 0.03        & \textbf{\underline{1.4}}  & -0.33           & 1.81                       & 0.95       \\ \midrule
            claude3 $Diff\_A$     & -394.5  & -11.5   & -37.5    & 60.0    & -0.37     & 0.09        & \textbf{\underline{4.66}} & -1.49           & 5.38                       & 2.62       \\ \midrule
            gemini $Diff\_N$      & -23.49  & 1.52    & -0.55    & -1.29   & -0.02     & -0.01       & 0.09                      & -0.05           & \textbf{\underline{6.41}}  & 0.2        \\ \midrule
            gemini $Diff\_A$      & -83.0   & 3.0     & -2.5     & -6.36   & -0.0      & -0.03       & 0.26                      & -0.08           & \textbf{\underline{19.63}} & 0.53       \\ \midrule
            gpt $Diff\_N$         & 13.66   & 2.83    & -3.65    & -3.19   & 0.04      & -0.04       & -0.24                     & -0.22           & 7.5                        & 0.0        \\ \midrule
            gpt $Diff\_A$         & 55.5    & 9.5     & -9.86    & -12.86  & 0.14      & -0.11       & -0.84                     & -0.22           & 18.67                      & -0.38      \\ \midrule
            command\_r $Diff\_N$  & -188.71 & 0.69    & -10.73   & 1.17    & -0.23     & -0.01       & \textbf{\underline{0.74}} & -0.83           & \textbf{\textit{-5.02}}    & -0.08      \\ \midrule
            command\_r $Diff\_A$  & -516.82 & 2.02    & -29.22   & 8.28    & -0.66     & -0.0        & \textbf{\underline{2.98}} & -2.48           & \textbf{\textit{-14.52}}   & -0.48      \\ \midrule
            llama3\_70b $Diff\_N$ & -202.95 & -7.84   & -13.73   & 1.43    & -0.15     & -0.04       & 0.38                      & -1.39           & 10.67                      & 1.37       \\ \midrule
            llama3\_70b $Diff\_A$ & -561.24 & -18.78  & -49.94   & 5.07    & -0.38     & -0.14       & 1.62                      & -4.81           & 29.26                      & 3.11       \\ \bottomrule
        \end{tabular}%
    }
    \caption{Yelp Results - Prompt With Examples\\Input External To Corpus}
    \label{table-prompt-2-yelp}
\end{table*}
\begin{table*}[!ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllllll}
            model                 & t\_word                   & t\_sent                 & n\_uverb                  & n\_uadj                  & simp\_ttr               & a\_verb\_pw & corr\_adj\_var          & corr\_verb\_var         & fkgl                    & a\_kup\_pw              \\
            \toprule
            claude3 $Diff\_N$     & -148.13                   & -1.63                   & -3.93                     & \textbf{\textit{-46.03}} & -0.11                   & -0.0        & \textbf{\textit{-3.08}} & -0.31                   & -4.3                    & \textbf{\textit{-1.53}} \\ \midrule
            claude3 $Diff\_A$     & -526.14                   & -7.98                   & -23.86                    & \textbf{\textit{-133.5}} & -0.41                   & -0.0        & \textbf{\textit{-8.98}} & -1.75                   & -21.03                  & \textbf{\textit{-5.34}} \\ \midrule
            gemini $Diff\_N$      & 74.27                     & 0.65                    & 4.67                      & 2.98                     & 0.04                    & -0.0        & -0.22                   & 1.05                    & 1.44                    & -0.65                   \\ \midrule
            gemini $Diff\_A$      & 221.0                     & 2.0                     & 10.5                      & 7.44                     & 0.12                    & -0.03       & -0.85                   & 2.73                    & 3.28                    & -1.79                   \\ \midrule
            gpt $Diff\_N$         & 0.65                      & 4.03                    & -1.19                     & -2.66                    & -0.03                   & -0.01       & -0.17                   & -0.13                   & 1.05                    & -0.67                   \\ \midrule
            gpt $Diff\_A$         & 17.0                      & 11.68                   & -3.68                     & -7.0                     & -0.1                    & -0.04       & -0.43                   & -0.05                   & 0.57                    & -1.75                   \\ \midrule
            command\_r $Diff\_N$  & \textbf{\textit{-236.11}} & \textbf{\textit{-3.6}}  & \textbf{\textit{-0.97}}   & \textbf{\textit{-27.04}} & \textbf{\textit{-0.23}} & 0.0         & -1.24                   & \textbf{\textit{-1.08}} & \textbf{\textit{-2.13}} & \textbf{\textit{-1.09}} \\ \midrule
            command\_r $Diff\_A$  & \textbf{\textit{-727.0}}  & \textbf{\textit{-9.24}} & \textbf{\underline{0.76}} & \textbf{\textit{-68.5}}  & \textbf{\textit{-0.64}} & 0.02        & -2.89                   & \textbf{\textit{-3.08}} & \textbf{\textit{-5.5}}  & \textbf{\textit{-3.0}}  \\ \midrule
            llama3\_70b $Diff\_N$ & \textbf{\textit{-333.03}} & \textbf{\textit{-5.19}} & 4.43                      & \textbf{\textit{-21.38}} & -0.16                   & -0.03       & \textbf{\textit{-1.77}} & -0.03                   & 1.87                    & -0.44                   \\ \midrule
            llama3\_70b $Diff\_A$ & \textbf{\textit{-687.34}} & \textbf{\textit{-3.42}} & 10.7                      & \textbf{\textit{-61.64}} & -0.31                   & -0.04       & \textbf{\textit{-5.6}}  & -0.89                   & 3.0                     & -1.92                   \\ \bottomrule
        \end{tabular}%
    }
    \caption{Yelp Results - Prompt With Examples\\Input From Corpus}
    \label{table-prompt-2-ifd-yelp}
\end{table*}

\section{Additional Plots}

\subsection{GPT}

\Cref{fig:gpt-prompt1-cnn,fig:gpt-prompt1-yelp,fig:gpt-prompt2-cnn,fig:gpt-prompt2-yelp,fig:gpt-prompt2-cnn-ifd,fig:gpt-prompt2-yelp-ifd}
show the experiment results for the Claude 3 Opus large language model.

\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-gpt-cnn_dailymail/prompt_1-gpt-cnn_dailymail.png}
    \caption{GPT on CNN Corpus\\Prompt Without Examples}
    \label{fig:gpt-prompt1-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-gpt-yelp/prompt_1-gpt-yelp.png}
    \caption{GPT on Yelp Corpus\\Prompt Without Examples}
    \label{fig:gpt-prompt1-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-gpt-cnn_dailymail/prompt_2-gpt-cnn_dailymail.png}
    \caption{GPT on CNN Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:gpt-prompt2-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-gpt-yelp/prompt_2-gpt-yelp.png}
    \caption{GPT on Yelp Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:gpt-prompt2-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-gpt-cnn_dailymail/prompt_2-gpt-cnn_dailymail.png}
    \caption{GPT on CNN Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:gpt-prompt2-cnn-ifd}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-gpt-yelp/prompt_2-gpt-yelp.png}
    \caption{GPT on Yelp Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:gpt-prompt2-yelp-ifd}
\end{figure*}

\subsection{Gemini}

\Cref{fig:gemini-prompt1-cnn,fig:gemini-prompt1-yelp,fig:gemini-prompt2-cnn,fig:gemini-prompt2-yelp,fig:gemini-prompt2-cnn-ifd,fig:gemini-prompt2-yelp-ifd}
show the experiment results for the Claude 3 Opus large language model.

\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-gemini-cnn_dailymail/prompt_1-gemini-cnn_dailymail.png}
    \caption{Gemini on CNN Corpus\\Prompt Without Examples}
    \label{fig:gemini-prompt1-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-gemini-yelp/prompt_1-gemini-yelp.png}
    \caption{Gemini on Yelp Corpus\\Prompt Without Examples}
    \label{fig:gemini-prompt1-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-gemini-cnn_dailymail/prompt_2-gemini-cnn_dailymail.png}
    \caption{Gemini on CNN Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:gemini-prompt2-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-gemini-yelp/prompt_2-gemini-yelp.png}
    \caption{Gemini on Yelp Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:gemini-prompt2-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-gemini-cnn_dailymail/prompt_2-gemini-cnn_dailymail.png}
    \caption{Gemini on CNN Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:gemini-prompt2-cnn-ifd}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-gemini-yelp/prompt_2-gemini-yelp.png}
    \caption{Gemini on Yelp Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:gemini-prompt2-yelp-ifd}
\end{figure*}

\subsection{Claude3}

\Cref{fig:claude3-prompt1-cnn,fig:claude3-prompt1-yelp,fig:claude3-prompt2-cnn,fig:claude3-prompt2-yelp,fig:claude3-prompt2-cnn-ifd,fig:claude3-prompt2-yelp-ifd}
show the experiment results for the Claude 3 Opus large language model.

\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-claude3-cnn_dailymail/prompt_1-claude3-cnn_dailymail.png}
    \caption{Claude3 on CNN Corpus\\Prompt Without Examples}
    \label{fig:claude3-prompt1-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-claude3-yelp/prompt_1-claude3-yelp.png}
    \caption{Claude3 on Yelp Corpus\\Prompt Without Examples}
    \label{fig:claude3-prompt1-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-claude3-cnn_dailymail/prompt_2-claude3-cnn_dailymail.png}
    \caption{Claude3 on CNN Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:claude3-prompt2-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-claude3-yelp/prompt_2-claude3-yelp.png}
    \caption{Claude3 on Yelp Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:claude3-prompt2-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-claude3-cnn_dailymail/prompt_2-claude3-cnn_dailymail.png}
    \caption{Claude3 on CNN Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:claude3-prompt2-cnn-ifd}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-claude3-yelp/prompt_2-claude3-yelp.png}
    \caption{Claude3 on Yelp Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:claude3-prompt2-yelp-ifd}
\end{figure*}

\subsection{Llama3}

\Cref{fig:llama3_70b-prompt1-cnn,fig:llama3_70b-prompt1-yelp,fig:llama3_70b-prompt2-cnn,fig:llama3_70b-prompt2-yelp,fig:llama3_70b-prompt2-cnn-ifd,fig:llama3_70b-prompt2-yelp-ifd}
show the experiment results for the Claude 3 Opus large language model.

\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-llama3_70b-cnn_dailymail/prompt_1-llama3_70b-cnn_dailymail.png}
    \caption{Llama3 on CNN Corpus\\Prompt Without Examples}
    \label{fig:llama3_70b-prompt1-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-llama3_70b-yelp/prompt_1-llama3_70b-yelp.png}
    \caption{Llama3 on Yelp Corpus\\Prompt Without Examples}
    \label{fig:llama3_70b-prompt1-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-llama3_70b-cnn_dailymail/prompt_2-llama3_70b-cnn_dailymail.png}
    \caption{Llama3 on CNN Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:llama3_70b-prompt2-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-llama3_70b-yelp/prompt_2-llama3_70b-yelp.png}
    \caption{Llama3 on Yelp Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:llama3_70b-prompt2-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-llama3_70b-cnn_dailymail/prompt_2-llama3_70b-cnn_dailymail.png}
    \caption{Llama3 on CNN Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:llama3_70b-prompt2-cnn-ifd}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-llama3_70b-yelp/prompt_2-llama3_70b-yelp.png}
    \caption{Llama3 on Yelp Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:llama3_70b-prompt2-yelp-ifd}
\end{figure*}

\subsection{Command R+}

\Cref{fig:command_r-prompt1-cnn,fig:command_r-prompt1-yelp,fig:command_r-prompt2-cnn,fig:command_r-prompt2-yelp,fig:command_r-prompt2-cnn-ifd,fig:command_r-prompt2-yelp-ifd}
show the experiment results for the Claude 3 Opus large language model.

\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-command_r-cnn_dailymail/prompt_1-command_r-cnn_dailymail.png}
    \caption{Command R on CNN Corpus\\Prompt Without Examples}
    \label{fig:command_r-prompt1-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_1/prompt_1-command_r-yelp/prompt_1-command_r-yelp.png}
    \caption{Command R on Yelp Corpus\\Prompt Without Examples}
    \label{fig:command_r-prompt1-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-command_r-cnn_dailymail/prompt_2-command_r-cnn_dailymail.png}
    \caption{Command R on CNN Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:command_r-prompt2-cnn}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2/prompt_2-command_r-yelp/prompt_2-command_r-yelp.png}
    \caption{Command R on Yelp Corpus\\Prompt With Examples\\Extraneous Input}
    \label{fig:command_r-prompt2-yelp}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-command_r-cnn_dailymail/prompt_2-command_r-cnn_dailymail.png}
    \caption{Command R on CNN Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:command_r-prompt2-cnn-ifd}
\end{figure*}
\begin{figure*}[ht]
    \includegraphics[width=\textwidth,height=0.9\textheight,scale=1]{plots/prompt_2_ifd/prompt_2-command_r-yelp/prompt_2-command_r-yelp.png}
    \caption{Command R on Yelp Corpus\\Prompt With Examples\\Input from Corpus}
    \label{fig:command_r-prompt2-yelp-ifd}
\end{figure*}

\end{document}