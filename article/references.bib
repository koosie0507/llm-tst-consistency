@article{tst_cl_survey_2022,
  author   = {Jin, Di and Jin, Zhijing and Hu, Zhiting and Vechtomova, Olga and Mihalcea, Rada},
  title    = {{Deep Learning for Text Style Transfer: A Survey}},
  journal  = {Computational Linguistics},
  volume   = {48},
  number   = {1},
  pages    = {155-205},
  year     = {2022},
  month    = {04},
  abstract = {{Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1}},
  issn     = {0891-2017},
  doi      = {10.1162/coli_a_00426},
  url      = {https://doi.org/10.1162/coli\_a\_00426},
  eprint   = {https://direct.mit.edu/coli/article-pdf/48/1/155/2006608/coli\_a\_00426.pdf}
}

@article{tst_sigkdd_review_2022,
  author     = {Hu, Zhiqiang and Lee, Roy Ka-Wei and Aggarwal, Charu C. and Zhang, Aston},
  title      = {Text Style Transfer: A Review and Experimental Evaluation},
  year       = {2022},
  issue_date = {June 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {1},
  issn       = {1931-0145},
  url        = {https://doi.org/10.1145/3544903.3544906},
  doi        = {10.1145/3544903.3544906},
  abstract   = {The stylistic properties of text have intrigued computational linguistics researchers in recent years. Specifically, researchers have investigated the text style transfer task (TST), which aims to change the stylistic properties of the text while retaining its independent content of style. Over the last few years, many novel TST algorithms have been developed, while the industry has leveraged these algorithms to enable exciting TST applications. The field of TST research has developed because of this symbiosis. This article aims to provide a comprehensive review of recent research efforts on text style transfer. More concretely, we create a taxonomy to organize the TST models, and provide a comprehensive summary of the state of the art. We review existing evaluation methodologies for TST tasks and conduct a large-scale reproducibility study in which we experimentally benchmark 19 state-of-the-art TST algorithms on two publicly available datasets. Finally, we expand on current trends and provide new perspectives on the new and exciting developments in the TST field.},
  journal    = {SIGKDD Explorations Newsletter},
  month      = {jun},
  pages      = {14--45},
  numpages   = {32}
}

@inproceedings{lftk-2023,
  title     = {{LFTK}: Handcrafted Features in Computational Linguistics},
  author    = {Lee, Bruce W.  and
               Lee, Jason},
  editor    = {Kochmar, Ekaterina  and
               Burstein, Jill  and
               Horbach, Andrea  and
               Laarmann-Quante, Ronja  and
               Madnani, Nitin  and
               Tack, Ana{\"\i}s  and
               Yaneva, Victoria  and
               Yuan, Zheng  and
               Zesch, Torsten},
  booktitle = {Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.bea-1.1},
  doi       = {10.18653/v1/2023.bea-1.1},
  pages     = {1--19},
  note      = {Version: 1.0.9}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{hf-transformers-2020,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas  and
               Debut, Lysandre  and
               Sanh, Victor  and
               Chaumond, Julien  and
               Delangue, Clement  and
               Moi, Anthony  and
               Cistac, Pierric  and
               Rault, Tim  and
               Louf, Remi  and
               Funtowicz, Morgan  and
               Davison, Joe  and
               Shleifer, Sam  and
               von Platen, Patrick  and
               Ma, Clara  and
               Jernite, Yacine  and
               Plu, Julien  and
               Xu, Canwen  and
               Le Scao, Teven  and
               Gugger, Sylvain  and
               Drame, Mariama  and
               Lhoest, Quentin  and
               Rush, Alexander},
  editor    = {Liu, Qun  and
               Schlangen, David},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-demos.6},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  pages     = {38--45},
  abstract  = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.}
}
@inproceedings{attention-2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}
@misc{gpt-2018,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year      = {2018},
  publisher = {OpenAI},
  url       = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  note      = {accessed on 2024-05-29}
}
@misc{gpt2-2019,
  title     = {Language Models are Unsupervised Multitask Learners},
  author    = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year      = {2019},
  publisher = {OpenAI},
  url       = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  note      = {accessed on 2024-05-29}
}
@article{gpt3-2020,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}
@ARTICLE{tst-review-2021,
  author={Toshevska, Martina and Gievska, Sonja},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={A Review of Text Style Transfer Using Deep Learning}, 
  year={2022},
  volume={3},
  number={5},
  pages={669-684},
  keywords={Writing;Deep learning;Natural language processing;Linguistics;Neural networks;Deep learning (DL);natural language generation (NLG);natural language processing;neural networks;text style transfer},
  doi={10.1109/TAI.2021.3115992}
}

@article{tst-survey-2022,
  title     = {Deep Learning for Text Style Transfer: A Survey},
  author    = {Jin, Di  and
               Jin, Zhijing  and
               Hu, Zhiting  and
               Vechtomova, Olga  and
               Mihalcea, Rada},
  journal   = {Computational Linguistics},
  volume    = {48},
  number    = {1},
  month     = mar,
  year      = {2022},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2022.cl-1.6},
  doi       = {10.1162/coli_a_00426},
  pages     = {155--205},
  abstract  = {Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1}
}

@article{gao2020pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@misc{zhao2023survey,
  title         = {A Survey of Large Language Models},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  eprint        = {2303.18223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{minaee2024llmsurvey,
  title         = {Large Language Models: A Survey},
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year          = {2024},
  eprint        = {2402.06196},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{papcode2024hellaswag,
  title  = {Sentence Completion on HellaSwag},
  year   = {2024},
  url    = {https://paperswithcode.com/sota/sentence-completion-on-hellaswag},
  note   = {accessed on 2024-05-29},
  author = {paperswithcode.com}
}

@inproceedings{yang2024unveiling,
    title = "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
    author = "Yang, Haoran  and
      Zhang, Yumeng  and
      Xu, Jiaqi  and
      Lu, Hongyuan  and
      Heng, Pheng-Ann  and
      Lam, Wai",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.51",
    doi = "10.18653/v1/2024.naacl-long.51",
    pages = "884--899",
    abstract = "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs{'} generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model{'}s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.",
}

@misc{chiang2024chatbot,
  title         = {Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author        = {Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
  year          = {2024},
  eprint        = {2403.04132},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@inproceedings{lee-etal-2021-pushing,
  title     = {Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features},
  author    = {Lee, Bruce W.  and
               Jang, Yoo Sung  and
               Lee, Jason},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.834},
  doi       = {10.18653/v1/2021.emnlp-main.834},
  pages     = {10669--10686}
}
@inproceedings{mcdonald1985computational,
  title     = {A computational theory of prose style for natural language generation},
  author    = {McDonald, David D and Pustejovsky, James},
  booktitle = {Second Conference of the European Chapter of the Association for Computational Linguistics},
  year      = {1985}
}
@article{hovy1987generating,
  title     = {Generating natural language under pragmatic constraints},
  author    = {Hovy, Eduard},
  journal   = {Journal of Pragmatics},
  volume    = {11},
  number    = {6},
  pages     = {689--719},
  year      = {1987},
  publisher = {Elsevier}
}
@article{dimarco1994model,
  title     = {A model of comparative stylistics for machine translation},
  author    = {DiMarco, Chrysanne and Mah, Keith},
  journal   = {Machine translation},
  volume    = {9},
  number    = {1},
  pages     = {21--59},
  year      = {1994},
  publisher = {Springer}
}
@book{chomsky2002syntactic,
  title     = {Syntactic structures},
  author    = {Chomsky, Noam},
  year      = {2002},
  publisher = {Mouton de Gruyter}
}
@inproceedings{lyu-etal-2021-styleptb,
  title     = {{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer},
  author    = {Lyu, Yiwei  and
               Liang, Paul Pu  and
               Pham, Hai  and
               Hovy, Eduard  and
               P{\'o}czos, Barnab{\'a}s  and
               Salakhutdinov, Ruslan  and
               Morency, Louis-Philippe},
  editor    = {Toutanova, Kristina  and
               Rumshisky, Anna  and
               Zettlemoyer, Luke  and
               Hakkani-Tur, Dilek  and
               Beltagy, Iz  and
               Bethard, Steven  and
               Cotterell, Ryan  and
               Chakraborty, Tanmoy  and
               Zhou, Yichao},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.naacl-main.171},
  doi       = {10.18653/v1/2021.naacl-main.171},
  pages     = {2116--2138}
}
@inproceedings{lyu-etal-2023-fine,
  title     = {Fine-grained Text Style Transfer with Diffusion-Based Language Models},
  author    = {Lyu, Yiwei  and
               Luo, Tiange  and
               Shi, Jiacheng  and
               Hollon, Todd  and
               Lee, Honglak},
  editor    = {Can, Burcu  and
               Mozes, Maximilian  and
               Cahyawijaya, Samuel  and
               Saphra, Naomi  and
               Kassner, Nora  and
               Ravfogel, Shauli  and
               Ravichander, Abhilasha  and
               Zhao, Chen  and
               Augenstein, Isabelle  and
               Rogers, Anna  and
               Cho, Kyunghyun  and
               Grefenstette, Edward  and
               Voita, Lena},
  booktitle = {Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.repl4nlp-1.6},
  doi       = {10.18653/v1/2023.repl4nlp-1.6},
  pages     = {65--74}
}

@article{zhang-ctg-2022,
  author     = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  title      = {A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3617680},
  doi        = {10.1145/3617680},
  abstract   = {Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
  journal    = {ACM Comput. Surv.},
  month      = {oct},
  articleno  = {64},
  numpages   = {37},
  keywords   = {systematic review, controllability, Transformer, pre-trained language models, Controllable text generation}
}

@article{chen2024benchmarking,
  author  = {Chen, Yihan and Xu, Benfeng and Wang, Quan and Liu, Yi and Mao, Zhendong},
  year    = {2024},
  month   = {03},
  pages   = {17808-17816},
  title   = {Benchmarking Large Language Models on Controllable Generation under Diversified Instructions},
  volume  = {38},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi     = {10.1609/aaai.v38i16.29734}
}

@article{verma2019lexical,
  title   = {A lexical, syntactic, and semantic perspective for understanding style in text},
  author  = {Verma, Gaurav and Srinivasan, Balaji Vasan},
  journal = {arXiv preprint arXiv:1909.08349},
  year    = {2019}
}

@inproceedings{wegmann-etal-2022-author,
  title     = {Same Author or Just Same Topic? Towards Content-Independent Style Representations},
  author    = {Wegmann, Anna  and
               Schraagen, Marijn  and
               Nguyen, Dong},
  editor    = {Gella, Spandana  and
               He, He  and
               Majumder, Bodhisattwa Prasad  and
               Can, Burcu  and
               Giunchiglia, Eleonora  and
               Cahyawijaya, Samuel  and
               Min, Sewon  and
               Mozes, Maximilian  and
               Li, Xiang Lorraine  and
               Augenstein, Isabelle  and
               Rogers, Anna  and
               Cho, Kyunghyun  and
               Grefenstette, Edward  and
               Rimell, Laura  and
               Dyer, Chris},
  booktitle = {Proceedings of the 7th Workshop on Representation Learning for NLP},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.repl4nlp-1.26},
  doi       = {10.18653/v1/2022.repl4nlp-1.26},
  pages     = {249--268}
}

@book{lugea2023stylistics,
  title     = {Stylistics: Text, Cognition and Corpora},
  author    = {Lugea, Jane and Walker, Brian},
  year      = {2023},
  month     = {october},
  publisher = {Palgrave Macmillan Cham},
  doi       = {10.1007/978-3-031-10422-0},
  isbn      = {978-3-031-10422-0}
}

@inproceedings{yelp2015neurips,
  author    = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Character-level Convolutional Networks for Text Classification},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
  volume    = {28},
  year      = {2015}
}

@inproceedings{cnndm2017,
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  author    = {See, Abigail  and
               Liu, Peter J.  and
               Manning, Christopher D.},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/P17-1099},
  doi       = {10.18653/v1/P17-1099},
  pages     = {1073--1083}
}

@inproceedings{cnndm2015,
  author    = {Karl Moritz Hermann and Tomás Kociský and Edward Grefenstette and Lasse Espeholt and Will Kay and Mustafa Suleyman and Phil Blunsom},
  title     = {Teaching Machines to Read and Comprehend},
  year      = {2015},
  cdate     = {1420070400000},
  pages     = {1693-1701},
  url       = {http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend},
  booktitle = {NIPS}
}

@inproceedings{lhoest-etal-2021-datasets,
  title         = {Datasets: A Community Library for Natural Language Processing},
  author        = {Lhoest, Quentin  and
                   Villanova del Moral, Albert  and
                   Jernite, Yacine  and
                   Thakur, Abhishek  and
                   von Platen, Patrick  and
                   Patil, Suraj  and
                   Chaumond, Julien  and
                   Drame, Mariama  and
                   Plu, Julien  and
                   Tunstall, Lewis  and
                   Davison, Joe  and
                   {\v{S}}a{\v{s}}ko, Mario  and
                   Chhablani, Gunjan  and
                   Malik, Bhavitvya  and
                   Brandeis, Simon  and
                   Le Scao, Teven  and
                   Sanh, Victor  and
                   Xu, Canwen  and
                   Patry, Nicolas  and
                   McMillan-Major, Angelina  and
                   Schmid, Philipp  and
                   Gugger, Sylvain  and
                   Delangue, Cl{\'e}ment  and
                   Matussi{\`e}re, Th{\'e}o  and
                   Debut, Lysandre  and
                   Bekman, Stas  and
                   Cistac, Pierric  and
                   Goehringer, Thibault  and
                   Mustar, Victor  and
                   Lagunas, Fran{\c{c}}ois  and
                   Rush, Alexander  and
                   Wolf, Thomas},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month         = nov,
  year          = {2021},
  address       = {Online and Punta Cana, Dominican Republic},
  publisher     = {Association for Computational Linguistics},
  url           = {https://aclanthology.org/2021.emnlp-demo.21},
  pages         = {175--184},
  abstract      = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
  eprint        = {2109.02846},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@book{sterne2003life,
  title     = {The Life and Opinions of Tristram Shandy, Gentleman},
  author    = {Sterne, L. and New, J. and New, M. and Ricks, C.},
  isbn      = {9780141920146},
  series    = {Penguin classics},
  url       = {https://books.google.ro/books?id=GGHGkzdohUgC},
  year      = {2003},
  publisher = {Penguin Books Limited}
}

@book{wilde1909poems,
  title     = {Poems: With the Ballad of Reading Gaol},
  author    = {Wilde, O.},
  url       = {https://books.google.ro/books?id=ahaDygAACAAJ},
  year      = {1909},
  publisher = {Methuen \& Company}
}

@article{kolmogorov1933,
  title   = {Sulla determinazione empirica di una legge didistribuzione},
  author  = {An, Kolmogorov},
  journal = {Giorn Dell'inst Ital Degli Att},
  volume  = {4},
  pages   = {89--91},
  year    = {1933}
}

@article{smirnov1939,
  title   = {On the estimation of the discrepancy between empirical curves of distribution for two independent samples},
  author  = {Smirnov, Nikolai V},
  journal = {Bull. Math. Univ. Moscou},
  volume  = {2},
  number  = {2},
  pages   = {3--14},
  year    = {1939}
}

@misc{jinja2,
  title  = {Jinja - Jinja Documentation (3.1.x)},
  author = {The Pallets Projects},
  year   = 2024,
  url    = {https://jinja.palletsprojects.com/en/3.1.x/},
  notes  = {License: BSD License; Last Accessed: 2024-06-14;}
}

@misc{gutenberg,
  title  = {Project Gutenberg},
  author = {Hart, Michael},
  year   = {n.d.},
  url    = {https://www.gutenberg.org/},
  notes  = {Last Accessed: 2024-06-14;}
}

@article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
             Haberland, Matt and Reddy, Tyler and Cournapeau, David and
             Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
             Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
             Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
             Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
             Kern, Robert and Larson, Eric and Carey, C J and
             Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
             {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
             Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
             Harris, Charles R. and Archibald, Anne M. and
             Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
             {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
             Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
  note    = {Version: 1.13.1}
}

@article{harris2020array,
  title     = {Array programming with {NumPy}},
  author    = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
               van der Walt and Ralf Gommers and Pauli Virtanen and David
               Cournapeau and Eric Wieser and Julian Taylor and Sebastian
               Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
               and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
               Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
               R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
               G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
               Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
               Travis E. Oliphant},
  year      = {2020},
  month     = sep,
  journal   = {Nature},
  volume    = {585},
  number    = {7825},
  pages     = {357--362},
  doi       = {10.1038/s41586-020-2649-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1038/s41586-020-2649-2}
}

@misc{spacy,
  author = {
    Honnibal, Matthew and
    Montani, Ines and
    Van Landeghem, Sofie and
    Boyd, Adriane
  },
  doi    = {10.5281/zenodo.1212303},
  title  = {spaCy: Industrial-strength Natural Language Processing in Python},
  year   = {2020},
  url    = {https://github.com/explosion/spaCy},
  note   = {Version: 3.7.5; Last Accessed: 2024-06-14}
}
