@article{tst_cl_survey_2022,
  author   = {Jin, Di and Jin, Zhijing and Hu, Zhiting and Vechtomova, Olga and Mihalcea, Rada},
  title    = {{Deep Learning for Text Style Transfer: A Survey}},
  journal  = {Computational Linguistics},
  volume   = {48},
  number   = {1},
  pages    = {155-205},
  year     = {2022},
  month    = {04},
  abstract = {{Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1}},
  issn     = {0891-2017},
  doi      = {10.1162/coli_a_00426},
  url      = {https://doi.org/10.1162/coli\_a\_00426},
  eprint   = {https://direct.mit.edu/coli/article-pdf/48/1/155/2006608/coli\_a\_00426.pdf}
}

@article{tst_sigkdd_review_2022,
  author     = {Hu, Zhiqiang and Lee, Roy Ka-Wei and Aggarwal, Charu C. and Zhang, Aston},
  title      = {Text Style Transfer: A Review and Experimental Evaluation},
  year       = {2022},
  issue_date = {June 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {1},
  issn       = {1931-0145},
  url        = {https://doi.org/10.1145/3544903.3544906},
  doi        = {10.1145/3544903.3544906},
  abstract   = {The stylistic properties of text have intrigued computational linguistics researchers in recent years. Specifically, researchers have investigated the text style transfer task (TST), which aims to change the stylistic properties of the text while retaining its independent content of style. Over the last few years, many novel TST algorithms have been developed, while the industry has leveraged these algorithms to enable exciting TST applications. The field of TST research has developed because of this symbiosis. This article aims to provide a comprehensive review of recent research efforts on text style transfer. More concretely, we create a taxonomy to organize the TST models, and provide a comprehensive summary of the state of the art. We review existing evaluation methodologies for TST tasks and conduct a large-scale reproducibility study in which we experimentally benchmark 19 state-of-the-art TST algorithms on two publicly available datasets. Finally, we expand on current trends and provide new perspectives on the new and exciting developments in the TST field.},
  journal    = {SIGKDD Explorations Newsletter},
  month      = {jun},
  pages      = {14--45},
  numpages   = {32}
}

@inproceedings{lftk-2023,
  title     = {{LFTK}: Handcrafted Features in Computational Linguistics},
  author    = {Lee, Bruce W.  and
               Lee, Jason},
  editor    = {Kochmar, Ekaterina  and
               Burstein, Jill  and
               Horbach, Andrea  and
               Laarmann-Quante, Ronja  and
               Madnani, Nitin  and
               Tack, Ana{\"\i}s  and
               Yaneva, Victoria  and
               Yuan, Zheng  and
               Zesch, Torsten},
  booktitle = {Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.bea-1.1},
  doi       = {10.18653/v1/2023.bea-1.1},
  pages     = {1--19}
}
@article{devlin2018bert,
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}
@inproceedings{hf-transformers-2020,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas  and
               Debut, Lysandre  and
               Sanh, Victor  and
               Chaumond, Julien  and
               Delangue, Clement  and
               Moi, Anthony  and
               Cistac, Pierric  and
               Rault, Tim  and
               Louf, Remi  and
               Funtowicz, Morgan  and
               Davison, Joe  and
               Shleifer, Sam  and
               von Platen, Patrick  and
               Ma, Clara  and
               Jernite, Yacine  and
               Plu, Julien  and
               Xu, Canwen  and
               Le Scao, Teven  and
               Gugger, Sylvain  and
               Drame, Mariama  and
               Lhoest, Quentin  and
               Rush, Alexander},
  editor    = {Liu, Qun  and
               Schlangen, David},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-demos.6},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  pages     = {38--45},
  abstract  = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.}
}
@inproceedings{attention-2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}
@misc{gpt-2018,
  title     = {Improving language understanding by generative pre-training},
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year      = {2018},
  publisher = {OpenAI},
  url       = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  note      = {accessed on 2024-05-29}
}
@misc{gpt2-2019,
  title     = {Language Models are Unsupervised Multitask Learners},
  author    = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year      = {2019},
  publisher = {OpenAI},
  url       = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  note      = {accessed on 2024-05-29}
}
@article{gpt3-2020,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}
@article{tst-review-2021,
  title   = {A Review of Text Style Transfer using Deep Learning},
  author  = {Toshevska, Martina and Gievska, Sonja},
  journal = {arXiv preprint arXiv:2109.15144},
  year    = {2021},
  url     = {https://arxiv.org/abs/2109.15144}
}
@article{tst-survey-2022,
  title     = {Deep Learning for Text Style Transfer: A Survey},
  author    = {Jin, Di  and
               Jin, Zhijing  and
               Hu, Zhiting  and
               Vechtomova, Olga  and
               Mihalcea, Rada},
  journal   = {Computational Linguistics},
  volume    = {48},
  number    = {1},
  month     = mar,
  year      = {2022},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2022.cl-1.6},
  doi       = {10.1162/coli_a_00426},
  pages     = {155--205},
  abstract  = {Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1}
}
@article{gao2020pile,
  title   = {The pile: An 800gb dataset of diverse text for language modeling},
  author  = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal = {arXiv preprint arXiv:2101.00027},
  year    = {2020}
}
@misc{zhao2023survey,
  title         = {A Survey of Large Language Models},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  year          = {2023},
  eprint        = {2303.18223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{minaee2024llmsurvey,
  title         = {Large Language Models: A Survey},
  author        = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year          = {2024},
  eprint        = {2402.06196},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{papcode2024hellaswag,
  title  = {Sentence Completion on HellaSwag},
  year   = {2024},
  url    = {https://paperswithcode.com/sota/sentence-completion-on-hellaswag},
  note   = {accessed on 2024-05-29},
  author = {paperswithcode.com}
}

@article{yang2024unveiling,
  title   = {Unveiling the Generalization Power of Fine-Tuned Large Language Models},
  author  = {Yang, Haoran and Zhang, Yumeng and Xu, Jiaqi and Lu, Hongyuan and Heng, Pheng Ann and Lam, Wai},
  journal = {arXiv preprint arXiv:2403.09162},
  year    = {2024}
}
@misc{chiang2024chatbot,
  title         = {Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
  author        = {Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
  year          = {2024},
  eprint        = {2403.04132},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@inproceedings{lee-etal-2021-pushing,
  title     = {Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features},
  author    = {Lee, Bruce W.  and
               Jang, Yoo Sung  and
               Lee, Jason},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.834},
  doi       = {10.18653/v1/2021.emnlp-main.834},
  pages     = {10669--10686}
}
@inproceedings{mcdonald1985computational,
  title     = {A computational theory of prose style for natural language generation},
  author    = {McDonald, David D and Pustejovsky, James},
  booktitle = {Second Conference of the European Chapter of the Association for Computational Linguistics},
  year      = {1985}
}
@article{hovy1987generating,
  title     = {Generating natural language under pragmatic constraints},
  author    = {Hovy, Eduard},
  journal   = {Journal of Pragmatics},
  volume    = {11},
  number    = {6},
  pages     = {689--719},
  year      = {1987},
  publisher = {Elsevier}
}
@article{dimarco1994model,
  title     = {A model of comparative stylistics for machine translation},
  author    = {DiMarco, Chrysanne and Mah, Keith},
  journal   = {Machine translation},
  volume    = {9},
  number    = {1},
  pages     = {21--59},
  year      = {1994},
  publisher = {Springer}
}
@book{chomsky2002syntactic,
  title     = {Syntactic structures},
  author    = {Chomsky, Noam},
  year      = {2002},
  publisher = {Mouton de Gruyter}
}
@inproceedings{lyu-etal-2021-styleptb,
  title     = {{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer},
  author    = {Lyu, Yiwei  and
               Liang, Paul Pu  and
               Pham, Hai  and
               Hovy, Eduard  and
               P{\'o}czos, Barnab{\'a}s  and
               Salakhutdinov, Ruslan  and
               Morency, Louis-Philippe},
  editor    = {Toutanova, Kristina  and
               Rumshisky, Anna  and
               Zettlemoyer, Luke  and
               Hakkani-Tur, Dilek  and
               Beltagy, Iz  and
               Bethard, Steven  and
               Cotterell, Ryan  and
               Chakraborty, Tanmoy  and
               Zhou, Yichao},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.naacl-main.171},
  doi       = {10.18653/v1/2021.naacl-main.171},
  pages     = {2116--2138}
}
@inproceedings{lyu-etal-2023-fine,
  title     = {Fine-grained Text Style Transfer with Diffusion-Based Language Models},
  author    = {Lyu, Yiwei  and
               Luo, Tiange  and
               Shi, Jiacheng  and
               Hollon, Todd  and
               Lee, Honglak},
  editor    = {Can, Burcu  and
               Mozes, Maximilian  and
               Cahyawijaya, Samuel  and
               Saphra, Naomi  and
               Kassner, Nora  and
               Ravfogel, Shauli  and
               Ravichander, Abhilasha  and
               Zhao, Chen  and
               Augenstein, Isabelle  and
               Rogers, Anna  and
               Cho, Kyunghyun  and
               Grefenstette, Edward  and
               Voita, Lena},
  booktitle = {Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.repl4nlp-1.6},
  doi       = {10.18653/v1/2023.repl4nlp-1.6},
  pages     = {65--74}
}

@article{zhang-ctg-2022,
  author     = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  title      = {A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3617680},
  doi        = {10.1145/3617680},
  abstract   = {Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
  journal    = {ACM Comput. Surv.},
  month      = {oct},
  articleno  = {64},
  numpages   = {37},
  keywords   = {systematic review, controllability, Transformer, pre-trained language models, Controllable text generation}
}

@article{chen2024benchmarking,
  author  = {Chen, Yihan and Xu, Benfeng and Wang, Quan and Liu, Yi and Mao, Zhendong},
  year    = {2024},
  month   = {03},
  pages   = {17808-17816},
  title   = {Benchmarking Large Language Models on Controllable Generation under Diversified Instructions},
  volume  = {38},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi     = {10.1609/aaai.v38i16.29734}
}